{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering_DistortedMetric.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNREZNi2JXaHqpVNVonD2Du",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maciejskorski/distorted_clustering/blob/main/Clustering_DistortedMetric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl8WB-phuGlj"
      },
      "source": [
        "# Summary\n",
        "\n",
        "This notebook demonstrates how to cluster under distorted metrics, like the huber-like loss functions. Such metrics have been recently shown to be statistically more robust and accuracte for various machine learning tasks (see the paper by Google https://arxiv.org/abs/1701.03077)\n",
        "\n",
        "Unfortunately, fitting such models is harder than for KMeans, and is not supported in off-the-shell software, such as scikitlearn. \n",
        "\n",
        "This notebook offers a Tensorflow implementation along with examples on real-world data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgnS4Eb2xmFe"
      },
      "source": [
        "# Model\n",
        "\n",
        "We use the Expectation-Maximization algorithm (EM). The idea is essentially as in the case of KMeans; however KMeans works under the euclidean distance and allows for solving the M step analytically. In our case we resort to generic (gradient-based) optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Ef7x5iBuVG"
      },
      "source": [
        "\n",
        "## Tensorflow 1.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ2IHGhwuAfM",
        "outputId": "a88bba5a-8c92-4417-dabe-93d94a9f0b14"
      },
      "source": [
        "### Tensorflow Model\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "def logp_huber(scale=1):\n",
        "\n",
        "  def logp_huber(x):\n",
        "    return -scale**2*((1+tf.square(x/scale))**0.5-1)\n",
        "  \n",
        "  return logp_huber\n",
        "\n",
        "def logp_euclidean():\n",
        "\n",
        "  def logp_euclidean(x):\n",
        "    return -tf.square(x)\n",
        "\n",
        "  return logp_euclidean\n",
        "\n",
        "def build_cluster_graph(data_shape,n_clusters,logp_func=logp_euclidean,init=None):\n",
        "\n",
        "  ## model variables \n",
        "\n",
        "  n_rows,n_features = data_shape\n",
        "  X_t = tf.placeholder(tf.float32,shape=[n_rows,n_features]) # data\n",
        "  y_logp = tf.Variable(tf.log(tf.ones(shape=[n_rows,n_clusters])/n_features),trainable=False) # class predictions, updated by Bayes rules\n",
        "  sigmas = tf.ones(shape=[n_features]) # standard deviations: here fixed\n",
        "  if init is None:\n",
        "    init = tf.random.normal(shape=[n_clusters,n_features])\n",
        "  centers = tf.Variable(init,trainable=True) # centers: trainable\n",
        "\n",
        "  ## data probability given class predictions and model params: log Pr[x|y,model]\n",
        "\n",
        "  diff = tf.expand_dims(X_t,axis=1) - tf.expand_dims(centers,0) # [n_rows,n_clusters,n_features]\n",
        "  X_logp = tf.reduce_sum(logp_func(diff),axis=2) # [n_rows,n_clusters]\n",
        "\n",
        "  ## optimize params given class predictions: log Pr[x|y]\n",
        "\n",
        "  loglike = tf.reduce_logsumexp(y_logp + X_logp,axis=1) # [n_rows]\n",
        "  neg_loglike = -tf.reduce_mean(loglike) # []\n",
        "  optimizer = tf.train.AdamOptimizer()\n",
        "  mstep = optimizer.minimize(neg_loglike)\n",
        "\n",
        "  ## optimize class predictions given params via Bayes: Pr[y|x] := Pr[x|y]Pr[y] / Pr[x] \n",
        "\n",
        "  y_logp_new = X_logp+y_logp - tf.reduce_logsumexp(X_logp+y_logp,axis=1,keepdims=True)\n",
        "  estep = tf.assign(y_logp,y_logp_new)\n",
        "\n",
        "  init_op = tf.global_variables_initializer()\n",
        "\n",
        "  return X_t,y_logp,mstep,estep,neg_loglike"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxklOEcryN2R"
      },
      "source": [
        "# Tests\n",
        "\n",
        "\n",
        "We demonstrate improvements when using Huber loss for clustering, \n",
        "on some standard clustering benchmarks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdkT65JiyPew"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.cluster import adjusted_rand_score,adjusted_mutual_info_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z2xtRHhybVX"
      },
      "source": [
        "## User Knowledge Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMdhzvLTywP0",
        "outputId": "ba6cd687-ecb9-4d95-f75d-7ed0115386b7"
      },
      "source": [
        "## download and prepare data\n",
        "\n",
        "!mkdir /content/user_knowledge\n",
        "!curl -o /content/user_knowledge/data.xls 'https://archive.ics.uci.edu/ml/machine-learning-databases/00257/Data_User_Modeling_Dataset_Hamdi%20Tolga%20KAHRAMAN.xls'\n",
        "X=pd.read_excel('user_knowledge/data.xls',sheet_name=1)\n",
        "X=X[X.columns[:6]]\n",
        "print(X)\n",
        "X=X.to_numpy()\n",
        "X,y=X[:,:-1],X[:,-1]\n",
        "X=X.astype('float')\n",
        "encode_dict={v:k for k,v in enumerate(np.unique(y))}\n",
        "y=np.array(list(encode_dict[i] for i in y))\n",
        "X=(X-X.mean(0))/X.std(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/user_knowledge’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 57856  100 57856    0     0   106k      0 --:--:-- --:--:-- --:--:--  105k\n",
            "      STG   SCG   STR   LPR   PEG       UNS\n",
            "0    0.00  0.00  0.00  0.00  0.00  very_low\n",
            "1    0.08  0.08  0.10  0.24  0.90      High\n",
            "2    0.06  0.06  0.05  0.25  0.33       Low\n",
            "3    0.10  0.10  0.15  0.65  0.30    Middle\n",
            "4    0.08  0.08  0.08  0.98  0.24       Low\n",
            "..    ...   ...   ...   ...   ...       ...\n",
            "253  0.61  0.78  0.69  0.92  0.58      High\n",
            "254  0.78  0.61  0.71  0.19  0.60    Middle\n",
            "255  0.54  0.82  0.71  0.29  0.77      High\n",
            "256  0.50  0.75  0.81  0.61  0.26    Middle\n",
            "257  0.66  0.90  0.76  0.87  0.74      High\n",
            "\n",
            "[258 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHQA_v2Ey2Q5",
        "outputId": "1fde65fc-de78-4d7d-8648-0ab0e66821f6"
      },
      "source": [
        "## cluster by KMeans\n",
        "\n",
        "np.random.seed(1234)\n",
        "y_pred = KMeans(n_clusters=len(np.unique(y)),n_init=10).fit_predict(X)\n",
        "print(adjusted_rand_score(y,y_pred),adjusted_mutual_info_score(y,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.16027687114838174 0.20584592211410666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "gSIgUF6hy9CB",
        "outputId": "89261302-c020-4720-8a50-198ac3c3137b"
      },
      "source": [
        "## cluster by Huber\n",
        "\n",
        "outs = []\n",
        "\n",
        "n_clusters=len(np.unique(y))\n",
        "n_rows,n_features = X.shape\n",
        "\n",
        "for _ in range(10):\n",
        "  \n",
        "  for logp_func in [logp_huber(0.25)]:\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    init = np.random.normal(size=(n_clusters,n_features)).astype(np.float32)\n",
        "    X_t,y_logp,mstep,estep,loglike = build_cluster_graph((n_rows,n_features),n_clusters,logp_func=logp_func,init=init)\n",
        "    init_op = tf.global_variables_initializer()\n",
        "\n",
        "    ## train\n",
        "    with tf.Session() as sess:\n",
        "      feed_dict = {X_t:X}\n",
        "      sess.run(init_op)\n",
        "      # do iterations\n",
        "      val = 0\n",
        "      for i in range(50):\n",
        "        # m step (many times to get close to maximum likelihood)\n",
        "        val_m = 0\n",
        "        for _ in range(1000):\n",
        "          _,val_m_new = sess.run([mstep,loglike],feed_dict=feed_dict)\n",
        "          if abs(val_m_new-val_m) < 1e-5:\n",
        "            break\n",
        "          else:\n",
        "            val_m = val_m_new\n",
        "        # e step (once as it is analytic)\n",
        "        sess.run(estep,feed_dict=feed_dict)\n",
        "\n",
        "      y_pred = sess.run(y_logp,feed_dict)\n",
        "\n",
        "    score1 = adjusted_rand_score(y,y_pred.argmax(1))\n",
        "    score2 = adjusted_mutual_info_score(y,y_pred.argmax(1))\n",
        "    outs.append((logp_func.__name__,val_m,score1,score2))\n",
        "    \n",
        "outs = pd.DataFrame(outs,columns=['method','loglike','ARI','AMI'])\n",
        "print(outs.sort_values('loglike')[:2].mean())\n",
        "outs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "loglike    0.555272\n",
            "ARI        0.189464\n",
            "AMI        0.212478\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>method</th>\n",
              "      <th>loglike</th>\n",
              "      <th>ARI</th>\n",
              "      <th>AMI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.571304</td>\n",
              "      <td>0.159812</td>\n",
              "      <td>0.212525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.559494</td>\n",
              "      <td>0.140852</td>\n",
              "      <td>0.180000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.557813</td>\n",
              "      <td>0.203783</td>\n",
              "      <td>0.292465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.553296</td>\n",
              "      <td>0.174851</td>\n",
              "      <td>0.195897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.560340</td>\n",
              "      <td>0.171611</td>\n",
              "      <td>0.237812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.569612</td>\n",
              "      <td>0.113819</td>\n",
              "      <td>0.136037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.579853</td>\n",
              "      <td>0.152204</td>\n",
              "      <td>0.183827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.557249</td>\n",
              "      <td>0.204077</td>\n",
              "      <td>0.229058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.559522</td>\n",
              "      <td>0.161576</td>\n",
              "      <td>0.220254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.561033</td>\n",
              "      <td>0.185360</td>\n",
              "      <td>0.226004</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       method   loglike       ARI       AMI\n",
              "0  logp_huber  0.571304  0.159812  0.212525\n",
              "1  logp_huber  0.559494  0.140852  0.180000\n",
              "2  logp_huber  0.557813  0.203783  0.292465\n",
              "3  logp_huber  0.553296  0.174851  0.195897\n",
              "4  logp_huber  0.560340  0.171611  0.237812\n",
              "5  logp_huber  0.569612  0.113819  0.136037\n",
              "6  logp_huber  0.579853  0.152204  0.183827\n",
              "7  logp_huber  0.557249  0.204077  0.229058\n",
              "8  logp_huber  0.559522  0.161576  0.220254\n",
              "9  logp_huber  0.561033  0.185360  0.226004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJtM_bup0lV2"
      },
      "source": [
        "## Mice Protein Nuclear Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZbNEo250n_x",
        "outputId": "08cd1177-f124-4bc0-ab94-45f711de07b4"
      },
      "source": [
        "## download and prepare data\n",
        "\n",
        "!mkdir /content/mice\n",
        "!curl -o /content/mice/data.xls 'https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls'\n",
        "\n",
        "X=pd.read_excel('mice/data.xls')\n",
        "# do one-hot-encoding for categorical data\n",
        "print(X.head())\n",
        "for c in X.columns[X.dtypes=='object']:\n",
        "  encode_dict = {v:k for k,v in enumerate(X[c].unique())}\n",
        "  X[c] = X[c].apply(encode_dict.get)\n",
        "# fill missing data\n",
        "X = X.fillna(X.median())\n",
        "X = X.to_numpy()\n",
        "X,y = X[:,:-1],X[:,-1]\n",
        "X = (X-X.mean(0))/X.std(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/mice’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1590k  100 1590k    0     0  2178k      0 --:--:-- --:--:-- --:--:-- 2175k\n",
            "  MouseID  DYRK1A_N   ITSN1_N    BDNF_N  ...  Genotype  Treatment  Behavior   class\n",
            "0   309_1  0.503644  0.747193  0.430175  ...   Control  Memantine       C/S  c-CS-m\n",
            "1   309_2  0.514617  0.689064  0.411770  ...   Control  Memantine       C/S  c-CS-m\n",
            "2   309_3  0.509183  0.730247  0.418309  ...   Control  Memantine       C/S  c-CS-m\n",
            "3   309_4  0.442107  0.617076  0.358626  ...   Control  Memantine       C/S  c-CS-m\n",
            "4   309_5  0.434940  0.617430  0.358802  ...   Control  Memantine       C/S  c-CS-m\n",
            "\n",
            "[5 rows x 82 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cMrSNRZ0ynR",
        "outputId": "f17ed481-a57d-40ec-92c6-a647505479ee"
      },
      "source": [
        "## cluster by KMeans\n",
        "\n",
        "np.random.seed(1234)\n",
        "y_pred = KMeans(n_clusters=len(np.unique(y)),n_init=10).fit_predict(X)\n",
        "print(adjusted_rand_score(y,y_pred),adjusted_rand_score(y,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1926732829622158 0.1926732829622158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "C4zDriha002Q",
        "outputId": "5831035a-ace1-4d0c-8aca-32b094921ad5"
      },
      "source": [
        "## cluster by Huber\n",
        "\n",
        "outs = []\n",
        "\n",
        "n_clusters=len(np.unique(y))\n",
        "n_rows,n_features = X.shape\n",
        "\n",
        "for _ in range(5):\n",
        "  \n",
        "  for logp_func in [logp_huber(0.25)]:\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    init = np.random.normal(size=(n_clusters,n_features)).astype(np.float32)\n",
        "    X_t,y_logp,mstep,estep,loglike = build_cluster_graph((n_rows,n_features),n_clusters,logp_func=logp_func,init=init)\n",
        "    init_op = tf.global_variables_initializer()\n",
        "\n",
        "    ## train\n",
        "    with tf.Session() as sess:\n",
        "      feed_dict = {X_t:X}\n",
        "      sess.run(init_op)\n",
        "      # do iterations\n",
        "      val = 0\n",
        "      for i in range(50):\n",
        "        # m step (many times to get close to maximum likelihood)\n",
        "        val_m = 0\n",
        "        for _ in range(1000):\n",
        "          _,val_m_new = sess.run([mstep,loglike],feed_dict=feed_dict)\n",
        "          if abs(val_m_new-val_m) < 1e-5:\n",
        "            break\n",
        "          else:\n",
        "            val_m = val_m_new\n",
        "        # e step (once as it is analytic)\n",
        "        sess.run(estep,feed_dict=feed_dict)\n",
        "\n",
        "      y_pred = sess.run(y_logp,feed_dict)\n",
        "\n",
        "    score = adjusted_rand_score(y,y_pred.argmax(1))\n",
        "    outs.append((logp_func.__name__,val_m,score))\n",
        "    \n",
        "outs = pd.DataFrame(outs,columns=['method','loglike','ARI'])\n",
        "print(outs.sort_values('loglike')[:2].mean())\n",
        "outs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loglike    8.109578\n",
            "ARI        0.206287\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>method</th>\n",
              "      <th>loglike</th>\n",
              "      <th>ARI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>8.159964</td>\n",
              "      <td>0.150249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>8.197568</td>\n",
              "      <td>0.228981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>8.098990</td>\n",
              "      <td>0.193778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>8.133040</td>\n",
              "      <td>0.223915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>8.120166</td>\n",
              "      <td>0.218797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       method   loglike       ARI\n",
              "0  logp_huber  8.159964  0.150249\n",
              "1  logp_huber  8.197568  0.228981\n",
              "2  logp_huber  8.098990  0.193778\n",
              "3  logp_huber  8.133040  0.223915\n",
              "4  logp_huber  8.120166  0.218797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt4zMf4qmhyr"
      },
      "source": [
        "## Alcohol Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMG75WnUmlvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8954f5-699a-45bb-dc70-08d1abfd7e5c"
      },
      "source": [
        "# download and prepare data\n",
        "\n",
        "!mkdir /content/alkoholqcm\n",
        "!curl -o /content/alkoholqcm/data.zip 'https://archive.ics.uci.edu/ml/machine-learning-databases/00496/QCM%20Sensor%20Alcohol%20Dataset.zip'\n",
        "!unzip -d /content/alkoholqcm  /content/alkoholqcm/data.zip\n",
        "\n",
        "import os\n",
        "out = []\n",
        "for fname in os.listdir('alkoholqcm/QCM Sensor Alcohol Dataset'):\n",
        "  out.append(pd.read_csv('alkoholqcm/QCM Sensor Alcohol Dataset/'+fname,sep=';',header=None,skiprows=1))\n",
        "X=pd.concat(out)\n",
        "print(X.head())\n",
        "X = X.to_numpy()\n",
        "X,y = X[:,:-5],X[:,-5:]\n",
        "y = y.argmax(1)\n",
        "X = (X-X.mean(0))/X.std(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/alkoholqcm’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5704  100  5704    0     0  20444      0 --:--:-- --:--:-- --:--:-- 20444\n",
            "Archive:  /content/alkoholqcm/data.zip\n",
            "replace /content/alkoholqcm/QCM Sensor Alcohol Dataset/QCM10.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: /content/alkoholqcm/QCM Sensor Alcohol Dataset/QCM10.csv  \n",
            "  inflating: /content/alkoholqcm/QCM Sensor Alcohol Dataset/QCM12.csv  \n",
            "  inflating: /content/alkoholqcm/QCM Sensor Alcohol Dataset/QCM3.csv  \n",
            "  inflating: /content/alkoholqcm/QCM Sensor Alcohol Dataset/QCM6.csv  \n",
            "  inflating: /content/alkoholqcm/QCM Sensor Alcohol Dataset/QCM7.csv  \n",
            "      0      1      2      3      4      5   ...      9   10  11  12  13  14\n",
            "0 -11.82 -13.29 -19.32 -26.28 -38.14 -50.09  ... -104.66   1   0   0   0   0\n",
            "1 -11.54 -14.18 -25.35 -32.75 -48.77 -60.08  ... -121.90   1   0   0   0   0\n",
            "2 -12.45 -15.81 -33.33 -40.64 -61.50 -72.12  ... -139.76   1   0   0   0   0\n",
            "3 -14.67 -18.49 -40.56 -47.67 -72.11 -82.19  ... -155.84   1   0   0   0   0\n",
            "4 -18.07 -19.74 -47.08 -53.50 -81.15 -89.81  ... -168.72   1   0   0   0   0\n",
            "\n",
            "[5 rows x 15 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvbYkGJ7pG_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dfb3db8-02ab-4649-d63b-aa8c2cddcf00"
      },
      "source": [
        "## cluster by KMeans\n",
        "\n",
        "np.random.seed(1234)\n",
        "y_pred = KMeans(n_clusters=len(np.unique(y)),n_init=10).fit_predict(X)\n",
        "print(adjusted_rand_score(y,y_pred),adjusted_mutual_info_score(y,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22795101368735207 0.297064834957504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0YsEYqNpIlw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "da3713c7-670a-4c34-a885-3a476e220899"
      },
      "source": [
        "## cluster by Huber\n",
        "\n",
        "outs = []\n",
        "\n",
        "n_clusters=len(np.unique(y))\n",
        "n_rows,n_features = X.shape\n",
        "\n",
        "for _ in range(5):\n",
        "  \n",
        "  for logp_func in [logp_huber(0.25)]:\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    init = np.random.normal(size=(n_clusters,n_features)).astype(np.float32)\n",
        "    X_t,y_logp,mstep,estep,loglike = build_cluster_graph((n_rows,n_features),n_clusters,logp_func=logp_func,init=init)\n",
        "    init_op = tf.global_variables_initializer()\n",
        "\n",
        "    ## train\n",
        "    with tf.Session() as sess:\n",
        "      feed_dict = {X_t:X}\n",
        "      sess.run(init_op)\n",
        "      # do iterations\n",
        "      val = 0\n",
        "      for i in range(50):\n",
        "        # m step (many times to get close to maximum likelihood)\n",
        "        val_m = 0\n",
        "        for _ in range(1000):\n",
        "          _,val_m_new = sess.run([mstep,loglike],feed_dict=feed_dict)\n",
        "          if abs(val_m_new-val_m) < 1e-5:\n",
        "            break\n",
        "          else:\n",
        "            val_m = val_m_new\n",
        "        # e step (once as it is analytic)\n",
        "        sess.run(estep,feed_dict=feed_dict)\n",
        "\n",
        "      y_pred = sess.run(y_logp,feed_dict)\n",
        "\n",
        "    score1 = adjusted_rand_score(y,y_pred.argmax(1))\n",
        "    score2 = adjusted_mutual_info_score(y,y_pred.argmax(1))\n",
        "    outs.append((logp_func.__name__,val_m,score1,score2))\n",
        "    \n",
        "outs = pd.DataFrame(outs,columns=['method','loglike','ARI','AMI'])\n",
        "print(outs.sort_values('loglike')[:2].mean())\n",
        "outs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loglike    0.294912\n",
            "ARI        0.260209\n",
            "AMI        0.329549\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>method</th>\n",
              "      <th>loglike</th>\n",
              "      <th>ARI</th>\n",
              "      <th>AMI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.297712</td>\n",
              "      <td>0.249461</td>\n",
              "      <td>0.317672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.296295</td>\n",
              "      <td>0.267094</td>\n",
              "      <td>0.335799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.296622</td>\n",
              "      <td>0.267094</td>\n",
              "      <td>0.335799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.306297</td>\n",
              "      <td>0.245432</td>\n",
              "      <td>0.330365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>logp_huber</td>\n",
              "      <td>0.293528</td>\n",
              "      <td>0.253324</td>\n",
              "      <td>0.323299</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       method   loglike       ARI       AMI\n",
              "0  logp_huber  0.297712  0.249461  0.317672\n",
              "1  logp_huber  0.296295  0.267094  0.335799\n",
              "2  logp_huber  0.296622  0.267094  0.335799\n",
              "3  logp_huber  0.306297  0.245432  0.330365\n",
              "4  logp_huber  0.293528  0.253324  0.323299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--1A_MgCB0wJ"
      },
      "source": [
        "# Experimental: Tensorflow 2.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jmw2Q_dB0OX"
      },
      "source": [
        "### Tensorflow Model\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "def logp_euclidean(x):\n",
        "  ''' probability based on euclidean distance '''\n",
        "  return -tf.square(x)\n",
        "\n",
        "def logp_huber(scale):\n",
        "  ''' probability based on huber distance '''\n",
        "\n",
        "  def logp_huber(x):\n",
        "    return -scale**2*((1+tf.square(x/scale))**0.5-1)\n",
        "\n",
        "  return logp_huber\n",
        "\n",
        "def train(X,n_clusters=3,logp_func=logp_euclidean,init=None):\n",
        "\n",
        "  ## read dataset description\n",
        "\n",
        "  n_rows,n_features = X.shape\n",
        "  n_clusters = 3\n",
        "\n",
        "  ## model variables\n",
        "\n",
        "  # cluster probabilities\n",
        "  y_logp = tf.Variable(tf.math.log(tf.ones(shape=[n_rows,n_clusters])/n_features))\n",
        "  # cluster variances, here fixed (simpler math)\n",
        "  sigmas = tf.ones(shape=[n_features]) # standard deviations: here fixed\n",
        "  # clusters centers\n",
        "  if init is None:\n",
        "    init = tf.random.normal(shape=[n_clusters,n_features])\n",
        "  centers = tf.Variable(init) # [n_clusters,n_features]\n",
        "\n",
        "  ## probability calculations, given cluster probabilities\n",
        "\n",
        "  def X_y_logp():\n",
        "    offset = tf.expand_dims(X,axis=1)  - tf.expand_dims(centers,0) # [n_rows,n_clusters,n_features]\n",
        "    return tf.reduce_sum(logp_func(offset),axis=2) # [n_rows,n_clusters]\n",
        "\n",
        "  def X_logp():\n",
        "    X_logp = X_y_logp() # [n_rows,n_clusters]\n",
        "    return tf.reduce_logsumexp(X_logp + y_logp,axis=1,keepdims=True) # [n_rows]\n",
        "\n",
        "  def logp():\n",
        "    return tf.reduce_mean(X_logp()) # []\n",
        "\n",
        "  ## update class probs (e-step)\n",
        "\n",
        "  @tf.function\n",
        "  def estep():\n",
        "    y_logp.assign(X_y_logp() + y_logp - X_logp())\n",
        "\n",
        "  ## update cluster locations (m-step)\n",
        "\n",
        "  optimizer = tf.optimizers.Adam()\n",
        "\n",
        "  @tf.function\n",
        "  def mstep():\n",
        "    optimizer.minimize(lambda: -logp(),[centers])\n",
        "\n",
        "  for _ in range(50):\n",
        "    m = 0\n",
        "    for _ in range(100):\n",
        "      mstep()\n",
        "      m_new = -logp()\n",
        "      if abs(m_new-m)<1e-4:\n",
        "        break\n",
        "      else:\n",
        "        m = m_new\n",
        "    estep()\n",
        "\n",
        "  return y_logp.numpy(),m.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsraKHaqf0GP",
        "outputId": "89de331b-094e-44c6-9b7c-975116dbaa56"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(666)\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "\n",
        "iris = datasets.load_wine()\n",
        "X = iris.data.astype('float32')\n",
        "y = iris.target.astype('int32')\n",
        "X = (X-X.mean(0))/X.std(0)\n",
        "\n",
        "n_features = X.shape[1]\n",
        "n_clusters = len(np.unique(y))\n",
        "\n",
        "for scale in [0.01,0.1,0.25,0.5,1,5]:\n",
        "  print(scale)\n",
        "  for _ in range(5):\n",
        "    init = np.random.normal(size=(n_clusters,n_features)).astype(np.float32)\n",
        "    y_pred,logp = train(X,n_clusters,logp_huber(scale),init)\n",
        "    print('ari=%s,logp=%s'%(adjusted_rand_score(y_pred.argmax(1),y),logp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01\n",
            "ari=0.0778678618108011,logp=0.11990287\n",
            "ari=0.3203694454096623,logp=0.113562495\n",
            "ari=0.24801772963076452,logp=0.120621316\n",
            "ari=0.17445954160586385,logp=0.13294195\n",
            "ari=0.2437744669020708,logp=0.12638737\n",
            "0.1\n",
            "ari=0.7537819920705809,logp=0.64651144\n",
            "ari=0.3755508420147064,logp=0.7114152\n",
            "ari=0.6644891723287554,logp=0.6498259\n",
            "ari=0.4278674623767463,logp=0.7699368\n",
            "ari=0.8162825428066811,logp=0.6710881\n",
            "0.25\n",
            "ari=0.8000727134982856,logp=1.2828132\n",
            "ari=0.8368297906511479,logp=1.3285872\n",
            "ari=0.689578596531488,logp=1.3171464\n",
            "ari=0.3238745336131592,logp=1.5650926\n",
            "ari=0.8637747871659045,logp=1.278671\n",
            "0.5\n",
            "ari=0.45577731264146465,logp=2.4726663\n",
            "ari=0.7002911871348606,logp=1.9996917\n",
            "ari=0.912130038265503,logp=1.964745\n",
            "ari=0.6052246062377723,logp=2.0473838\n",
            "ari=0.7711034095636224,logp=2.0088317\n",
            "1\n",
            "ari=0.8313336205700521,logp=2.6896696\n",
            "ari=0.3698552896570588,logp=3.3406065\n",
            "ari=0.8791908099398843,logp=2.690258\n",
            "ari=0.8158755966377977,logp=2.706981\n",
            "ari=0.8150068438497655,logp=2.6871662\n",
            "5\n",
            "ari=0.6862401409895932,logp=3.672369\n",
            "ari=0.8303725082480321,logp=3.5812182\n",
            "ari=0.7726756627416556,logp=3.5793245\n",
            "ari=0.8625485319615126,logp=3.557266\n",
            "ari=0.7141404451210713,logp=3.5941546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6UGE195WLPU",
        "outputId": "95a5b154-0e45-42df-dd87-9f64a8c0a27c"
      },
      "source": [
        "from sklearn import cluster\n",
        "\n",
        "adjusted_rand_score(cluster.KMeans(n_clusters).fit_predict(X),y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8974949815093207"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    }
  ]
}